==PROF== Connected to process 203345 (/usr/bin/python3.10)
==PROF== Profiling "ampere_sgemm_32x32_sliced1x4_nn": 0%....50%....100% - 34 passes
==PROF== Profiling "ampere_sgemm_32x32_sliced1x4_nn": 0%....50%....100% - 34 passes
==PROF== Profiling "ampere_sgemm_32x32_sliced1x4_nn": 0%....50%....100% - 34 passes
==PROF== Profiling "ampere_sgemm_32x32_sliced1x4_nn": 0%....50%....100% - 34 passes
==PROF== Profiling "ampere_sgemm_32x32_sliced1x4_nn": 0%....50%....100% - 34 passes
==PROF== Profiling "ampere_sgemm_32x32_sliced1x4_nn": 0%....50%....100% - 34 passes
==PROF== Profiling "ampere_sgemm_32x32_sliced1x4_nn": 0%....50%....100% - 34 passes
==PROF== Profiling "ampere_sgemm_32x32_sliced1x4_nn": 0%....50%....100% - 34 passes
==PROF== Profiling "ampere_sgemm_32x32_sliced1x4_nn": 0%....50%....100% - 34 passes
==PROF== Profiling "ampere_sgemm_32x32_sliced1x4_nn": 0%....50%....100% - 34 passes
==PROF== Disconnected from process 203345
[203345] python3.10@127.0.0.1
  ampere_sgemm_32x32_sliced1x4_nn (1, 4, 13)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.93
    SM Frequency            cycle/usecond       889.63
    Elapsed Cycles                  cycle       14,719
    Memory Throughput                   %        36.75
    DRAM Throughput                     %        21.60
    Duration                      usecond        16.54
    L1/TEX Cache Throughput             %        44.08
    L2 Cache Throughput                 %        16.54
    SM Active Cycles                cycle    12,270.03
    Compute (SM) Throughput             %        35.52
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 15%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.40
    Executed Ipc Elapsed  inst/cycle         1.16
    Issue Slots Busy               %        35.09
    Issued Ipc Active     inst/cycle         1.40
    SM Busy                        %        35.09
    -------------------- ----------- ------------

    WRN   All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        71.32
    Mem Busy                               %        36.75
    Max Bandwidth                          %        35.52
    L1/TEX Hit Rate                        %         7.57
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        58.44
    Mem Pipes Busy                         %        35.52
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.46
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.54
    Active Warps Per Scheduler          warp         1.75
    Eligible Warps Per Scheduler        warp         0.44
    ---------------------------- ----------- ------------

    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.75 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         4.94
    Warp Cycles Per Executed Instruction           cycle         4.97
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.52
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     4,283.07
    Executed Instructions                           inst      513,968
    Avg. Issued Instructions Per Scheduler          inst     4,305.60
    Issued Instructions                             inst      516,672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     52
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           32.77
    Threads                                   thread           6,656
    Waves Per SM                                                0.58
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        14.46
    Achieved Active Warps Per SM           warp         6.94
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The           
          difference between calculated theoretical (25.0%) and measured achieved occupancy (14.5%) can be the result   
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       12,896
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    WRN   This kernel has uncoalesced global accesses resulting in a total of 5376 excessive sectors (9% of the total   
          58240 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.     
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 36608 excessive wavefronts (30% of the    
          total 122928 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.    
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  ampere_sgemm_32x32_sliced1x4_nn (1, 4, 13)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.73
    SM Frequency            cycle/usecond       864.78
    Elapsed Cycles                  cycle       14,419
    Memory Throughput                   %        37.51
    DRAM Throughput                     %        22.04
    Duration                      usecond        16.67
    L1/TEX Cache Throughput             %        45.39
    L2 Cache Throughput                 %        16.90
    SM Active Cycles                cycle    11,915.13
    Compute (SM) Throughput             %        36.26
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 16%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.44
    Executed Ipc Elapsed  inst/cycle         1.19
    Issue Slots Busy               %        36.14
    Issued Ipc Active     inst/cycle         1.45
    SM Busy                        %        36.14
    -------------------- ----------- ------------

    WRN   All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        70.77
    Mem Busy                               %        37.51
    Max Bandwidth                          %        36.26
    L1/TEX Hit Rate                        %         7.50
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        56.02
    Mem Pipes Busy                         %        36.26
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        36.07
    Issued Warp Per Scheduler                        0.36
    No Eligible                            %        63.93
    Active Warps Per Scheduler          warp         1.76
    Eligible Warps Per Scheduler        warp         0.44
    ---------------------------- ----------- ------------

    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.76 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         4.88
    Warp Cycles Per Executed Instruction           cycle         4.91
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.52
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     4,283.07
    Executed Instructions                           inst      513,968
    Avg. Issued Instructions Per Scheduler          inst     4,305.60
    Issued Instructions                             inst      516,672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     52
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           32.77
    Threads                                   thread           6,656
    Waves Per SM                                                0.58
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        14.82
    Achieved Active Warps Per SM           warp         7.11
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The           
          difference between calculated theoretical (25.0%) and measured achieved occupancy (14.8%) can be the result   
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       12,896
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    WRN   This kernel has uncoalesced global accesses resulting in a total of 5376 excessive sectors (9% of the total   
          58240 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.     
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 36608 excessive wavefronts (30% of the    
          total 122928 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.    
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  ampere_sgemm_32x32_sliced1x4_nn (1, 4, 13)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.92
    SM Frequency            cycle/usecond       890.14
    Elapsed Cycles                  cycle       14,756
    Memory Throughput                   %        36.64
    DRAM Throughput                     %        22.02
    Duration                      usecond        16.58
    L1/TEX Cache Throughput             %        45.08
    L2 Cache Throughput                 %        18.19
    SM Active Cycles                cycle    11,992.57
    Compute (SM) Throughput             %        35.43
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 15%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.43
    Executed Ipc Elapsed  inst/cycle         1.16
    Issue Slots Busy               %        35.90
    Issued Ipc Active     inst/cycle         1.44
    SM Busy                        %        35.90
    -------------------- ----------- ------------

    WRN   All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        72.73
    Mem Busy                               %        36.64
    Max Bandwidth                          %        35.43
    L1/TEX Hit Rate                        %         7.45
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        56.15
    Mem Pipes Busy                         %        35.43
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        34.61
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        65.39
    Active Warps Per Scheduler          warp         1.69
    Eligible Warps Per Scheduler        warp         0.43
    ---------------------------- ----------- ------------

    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.69 active warps per scheduler, but only an average of 0.43 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         4.88
    Warp Cycles Per Executed Instruction           cycle         4.91
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.52
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     4,283.07
    Executed Instructions                           inst      513,968
    Avg. Issued Instructions Per Scheduler          inst     4,305.60
    Issued Instructions                             inst      516,672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     52
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           32.77
    Threads                                   thread           6,656
    Waves Per SM                                                0.58
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        14.78
    Achieved Active Warps Per SM           warp         7.09
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The           
          difference between calculated theoretical (25.0%) and measured achieved occupancy (14.8%) can be the result   
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       12,896
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    WRN   This kernel has uncoalesced global accesses resulting in a total of 5376 excessive sectors (9% of the total   
          58240 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.     
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 36608 excessive wavefronts (30% of the    
          total 122928 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.    
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  ampere_sgemm_32x32_sliced1x4_nn (1, 4, 13)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.83
    SM Frequency            cycle/usecond       881.03
    Elapsed Cycles                  cycle       14,802
    Memory Throughput                   %        36.53
    DRAM Throughput                     %        21.79
    Duration                      usecond        16.80
    L1/TEX Cache Throughput             %        44.68
    L2 Cache Throughput                 %        16.43
    SM Active Cycles                cycle    12,099.33
    Compute (SM) Throughput             %        35.32
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 15%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.42
    Executed Ipc Elapsed  inst/cycle         1.16
    Issue Slots Busy               %        35.59
    Issued Ipc Active     inst/cycle         1.42
    SM Busy                        %        35.59
    -------------------- ----------- ------------

    WRN   All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        71.22
    Mem Busy                               %        36.53
    Max Bandwidth                          %        35.32
    L1/TEX Hit Rate                        %         7.34
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        56.07
    Mem Pipes Busy                         %        35.32
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        36.04
    Issued Warp Per Scheduler                        0.36
    No Eligible                            %        63.96
    Active Warps Per Scheduler          warp         1.76
    Eligible Warps Per Scheduler        warp         0.44
    ---------------------------- ----------- ------------

    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.76 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         4.87
    Warp Cycles Per Executed Instruction           cycle         4.90
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.52
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     4,283.07
    Executed Instructions                           inst      513,968
    Avg. Issued Instructions Per Scheduler          inst     4,305.60
    Issued Instructions                             inst      516,672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     52
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           32.77
    Threads                                   thread           6,656
    Waves Per SM                                                0.58
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        14.60
    Achieved Active Warps Per SM           warp         7.01
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The           
          difference between calculated theoretical (25.0%) and measured achieved occupancy (14.6%) can be the result   
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       12,896
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    WRN   This kernel has uncoalesced global accesses resulting in a total of 5376 excessive sectors (9% of the total   
          58240 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.     
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 36608 excessive wavefronts (30% of the    
          total 122928 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.    
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  ampere_sgemm_32x32_sliced1x4_nn (1, 4, 13)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.67
    SM Frequency            cycle/usecond       859.61
    Elapsed Cycles                  cycle       14,580
    Memory Throughput                   %        37.08
    DRAM Throughput                     %        21.82
    Duration                      usecond        16.96
    L1/TEX Cache Throughput             %        44.85
    L2 Cache Throughput                 %        16.68
    SM Active Cycles                cycle    12,054.33
    Compute (SM) Throughput             %        35.86
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 16%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.42
    Executed Ipc Elapsed  inst/cycle         1.18
    Issue Slots Busy               %        35.72
    Issued Ipc Active     inst/cycle         1.43
    SM Busy                        %        35.72
    -------------------- ----------- ------------

    WRN   All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        69.57
    Mem Busy                               %        37.08
    Max Bandwidth                          %        35.86
    L1/TEX Hit Rate                        %         7.44
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        56.26
    Mem Pipes Busy                         %        35.86
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.61
    Issued Warp Per Scheduler                        0.36
    No Eligible                            %        64.39
    Active Warps Per Scheduler          warp         1.77
    Eligible Warps Per Scheduler        warp         0.44
    ---------------------------- ----------- ------------

    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.77 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         4.97
    Warp Cycles Per Executed Instruction           cycle         4.99
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.52
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     4,283.07
    Executed Instructions                           inst      513,968
    Avg. Issued Instructions Per Scheduler          inst     4,305.60
    Issued Instructions                             inst      516,672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     52
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           32.77
    Threads                                   thread           6,656
    Waves Per SM                                                0.58
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        15.17
    Achieved Active Warps Per SM           warp         7.28
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. See the CUDA  
          Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for      
          more details on optimizing occupancy.                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       12,896
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    WRN   This kernel has uncoalesced global accesses resulting in a total of 5376 excessive sectors (9% of the total   
          58240 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.     
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 36608 excessive wavefronts (30% of the    
          total 122928 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.    
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  ampere_sgemm_32x32_sliced1x4_nn (1, 4, 13)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.05
    SM Frequency            cycle/usecond       907.34
    Elapsed Cycles                  cycle       15,216
    Memory Throughput                   %        35.52
    DRAM Throughput                     %        29.59
    Duration                      usecond        16.77
    L1/TEX Cache Throughput             %        45.12
    L2 Cache Throughput                 %        15.98
    SM Active Cycles                cycle    11,977.53
    Compute (SM) Throughput             %        34.36
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 15%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.43
    Executed Ipc Elapsed  inst/cycle         1.13
    Issue Slots Busy               %        35.95
    Issued Ipc Active     inst/cycle         1.44
    SM Busy                        %        35.95
    -------------------- ----------- ------------

    WRN   All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        99.63
    Mem Busy                               %        35.52
    Max Bandwidth                          %        34.36
    L1/TEX Hit Rate                        %         7.50
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        56.33
    Mem Pipes Busy                         %        34.36
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.05
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.95
    Active Warps Per Scheduler          warp         1.72
    Eligible Warps Per Scheduler        warp         0.43
    ---------------------------- ----------- ------------

    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.72 active warps per scheduler, but only an average of 0.43 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         4.90
    Warp Cycles Per Executed Instruction           cycle         4.92
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.52
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     4,283.07
    Executed Instructions                           inst      513,968
    Avg. Issued Instructions Per Scheduler          inst     4,305.60
    Issued Instructions                             inst      516,672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     52
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           32.77
    Threads                                   thread           6,656
    Waves Per SM                                                0.58
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        14.68
    Achieved Active Warps Per SM           warp         7.05
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The           
          difference between calculated theoretical (25.0%) and measured achieved occupancy (14.7%) can be the result   
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       12,896
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    WRN   This kernel has uncoalesced global accesses resulting in a total of 5376 excessive sectors (9% of the total   
          58240 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.     
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 36608 excessive wavefronts (30% of the    
          total 122928 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.    
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  ampere_sgemm_32x32_sliced1x4_nn (1, 4, 13)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.03
    SM Frequency            cycle/usecond       905.34
    Elapsed Cycles                  cycle       15,298
    Memory Throughput                   %        36.35
    DRAM Throughput                     %        36.35
    Duration                      usecond        16.90
    L1/TEX Cache Throughput             %        43.71
    L2 Cache Throughput                 %        19.57
    SM Active Cycles                cycle    12,383.87
    Compute (SM) Throughput             %        34.18
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 15%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.38
    Executed Ipc Elapsed  inst/cycle         1.12
    Issue Slots Busy               %        34.77
    Issued Ipc Active     inst/cycle         1.39
    SM Busy                        %        34.77
    -------------------- ----------- ------------

    WRN   All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second       122.14
    Mem Busy                               %        35.38
    Max Bandwidth                          %        36.35
    L1/TEX Hit Rate                        %         7.49
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        47.07
    Mem Pipes Busy                         %        34.18
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.47
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        64.53
    Active Warps Per Scheduler          warp         1.81
    Eligible Warps Per Scheduler        warp         0.44
    ---------------------------- ----------- ------------

    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.81 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         5.11
    Warp Cycles Per Executed Instruction           cycle         5.13
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.52
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     4,283.07
    Executed Instructions                           inst      513,968
    Avg. Issued Instructions Per Scheduler          inst     4,305.60
    Issued Instructions                             inst      516,672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     52
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           32.77
    Threads                                   thread           6,656
    Waves Per SM                                                0.58
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        14.49
    Achieved Active Warps Per SM           warp         6.96
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The           
          difference between calculated theoretical (25.0%) and measured achieved occupancy (14.5%) can be the result   
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       12,896
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    WRN   This kernel has uncoalesced global accesses resulting in a total of 5376 excessive sectors (9% of the total   
          58240 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.     
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 36608 excessive wavefronts (30% of the    
          total 122928 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.    
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  ampere_sgemm_32x32_sliced1x4_nn (1, 4, 13)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.89
    SM Frequency            cycle/usecond       887.51
    Elapsed Cycles                  cycle       15,140
    Memory Throughput                   %        35.69
    DRAM Throughput                     %        29.44
    Duration                      usecond        17.06
    L1/TEX Cache Throughput             %        45.17
    L2 Cache Throughput                 %        16.07
    SM Active Cycles                cycle    11,961.23
    Compute (SM) Throughput             %        34.54
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 15%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.43
    Executed Ipc Elapsed  inst/cycle         1.13
    Issue Slots Busy               %        36.00
    Issued Ipc Active     inst/cycle         1.44
    SM Busy                        %        36.00
    -------------------- ----------- ------------

    WRN   All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        96.90
    Mem Busy                               %        35.69
    Max Bandwidth                          %        34.54
    L1/TEX Hit Rate                        %         7.42
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        55.96
    Mem Pipes Busy                         %        34.54
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.92
    Issued Warp Per Scheduler                        0.36
    No Eligible                            %        64.08
    Active Warps Per Scheduler          warp         1.86
    Eligible Warps Per Scheduler        warp         0.44
    ---------------------------- ----------- ------------

    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.86 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         5.19
    Warp Cycles Per Executed Instruction           cycle         5.22
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.52
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     4,283.07
    Executed Instructions                           inst      513,968
    Avg. Issued Instructions Per Scheduler          inst     4,305.60
    Issued Instructions                             inst      516,672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     52
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           32.77
    Threads                                   thread           6,656
    Waves Per SM                                                0.58
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        14.70
    Achieved Active Warps Per SM           warp         7.06
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The           
          difference between calculated theoretical (25.0%) and measured achieved occupancy (14.7%) can be the result   
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       12,896
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    WRN   This kernel has uncoalesced global accesses resulting in a total of 5376 excessive sectors (9% of the total   
          58240 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.     
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 36608 excessive wavefronts (30% of the    
          total 122928 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.    
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  ampere_sgemm_32x32_sliced1x4_nn (1, 4, 13)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.68
    SM Frequency            cycle/usecond       861.69
    Elapsed Cycles                  cycle       14,698
    Memory Throughput                   %        36.84
    DRAM Throughput                     %        21.64
    Duration                      usecond        17.06
    L1/TEX Cache Throughput             %        44.46
    L2 Cache Throughput                 %        21.48
    SM Active Cycles                cycle    12,180.10
    Compute (SM) Throughput             %        35.57
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 15%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.41
    Executed Ipc Elapsed  inst/cycle         1.17
    Issue Slots Busy               %        35.35
    Issued Ipc Active     inst/cycle         1.41
    SM Busy                        %        35.35
    -------------------- ----------- ------------

    WRN   All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second        69.18
    Mem Busy                               %        36.84
    Max Bandwidth                          %        35.57
    L1/TEX Hit Rate                        %         7.54
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        43.34
    Mem Pipes Busy                         %        35.57
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        35.96
    Issued Warp Per Scheduler                        0.36
    No Eligible                            %        64.04
    Active Warps Per Scheduler          warp         1.77
    Eligible Warps Per Scheduler        warp         0.44
    ---------------------------- ----------- ------------

    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.77 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         4.92
    Warp Cycles Per Executed Instruction           cycle         4.95
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.52
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     4,283.07
    Executed Instructions                           inst      513,968
    Avg. Issued Instructions Per Scheduler          inst     4,305.60
    Issued Instructions                             inst      516,672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     52
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           32.77
    Threads                                   thread           6,656
    Waves Per SM                                                0.58
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        14.66
    Achieved Active Warps Per SM           warp         7.04
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The           
          difference between calculated theoretical (25.0%) and measured achieved occupancy (14.7%) can be the result   
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       12,896
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    WRN   This kernel has uncoalesced global accesses resulting in a total of 5376 excessive sectors (9% of the total   
          58240 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.     
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 36608 excessive wavefronts (30% of the    
          total 122928 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.    
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

  ampere_sgemm_32x32_sliced1x4_nn (1, 4, 13)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         7.06
    SM Frequency            cycle/usecond       907.85
    Elapsed Cycles                  cycle       15,282
    Memory Throughput                   %        35.39
    DRAM Throughput                     %        35.35
    Duration                      usecond        16.83
    L1/TEX Cache Throughput             %        45.42
    L2 Cache Throughput                 %        15.89
    SM Active Cycles                cycle    11,906.20
    Compute (SM) Throughput             %        34.21
    ----------------------- ------------- ------------

    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 15%   
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.44
    Executed Ipc Elapsed  inst/cycle         1.12
    Issue Slots Busy               %        36.16
    Issued Ipc Active     inst/cycle         1.45
    SM Busy                        %        36.16
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (20.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ------------ ------------
    Metric Name                  Metric Unit Metric Value
    --------------------------- ------------ ------------
    Memory Throughput           Gbyte/second       119.16
    Mem Busy                               %        35.39
    Max Bandwidth                          %        35.35
    L1/TEX Hit Rate                        %         7.55
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Hit Rate                            %        56.25
    Mem Pipes Busy                         %        34.21
    --------------------------- ------------ ------------

    Section: Memory Workload Analysis Tables
    WRN   The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        36.30
    Issued Warp Per Scheduler                        0.36
    No Eligible                            %        63.70
    Active Warps Per Scheduler          warp         1.77
    Eligible Warps Per Scheduler        warp         0.45
    ---------------------------- ----------- ------------

    WRN   Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          1.77 active warps per scheduler, but only an average of 0.45 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical           
          occupancy.                                                                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         4.87
    Warp Cycles Per Executed Instruction           cycle         4.90
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.52
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     4,283.07
    Executed Instructions                           inst      513,968
    Avg. Issued Instructions Per Scheduler          inst     4,305.60
    Issued Instructions                             inst      516,672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     52
    Registers Per Thread             register/thread              86
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           32.77
    Threads                                   thread           6,656
    Waves Per SM                                                0.58
    -------------------------------- --------------- ---------------

    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %           25
    Achieved Occupancy                        %        14.63
    Achieved Active Warps Per SM           warp         7.02
    ------------------------------- ----------- ------------

    WRN   This kernel's theoretical occupancy (25.0%) is limited by the required amount of shared memory. The           
          difference between calculated theoretical (25.0%) and measured achieved occupancy (14.6%) can be the result   
          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    
          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst       12,896
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    WRN   This kernel has uncoalesced global accesses resulting in a total of 5376 excessive sectors (9% of the total   
          58240 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.     
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    WRN   This kernel has uncoalesced shared accesses resulting in a total of 36608 excessive wavefronts (30% of the    
          total 122928 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.    
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -aa) has an example on optimizing shared memory accesses.                                                     

